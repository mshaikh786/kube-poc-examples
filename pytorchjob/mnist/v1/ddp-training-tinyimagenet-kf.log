L-3 : W-4 , R-3 

L-1 : W-4 , R-1 

L-0 : W-4 , R-0 

L-2 : W-4 , R-2 

PATH: /opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME: dist-worker-0
NVIDIA_VISIBLE_DEVICES: GPU-85172998-46db-3656-f3f1-cb076a965d80,GPU-eebefd56-1ec7-b5f5-bfc5-cf8412b2aae3,GPU-15b5a5c6-cbc7-6aa5-b4d2-4df7e513ba1f,GPU-dd663deb-dee4-f5fb-d516-cd9012bbc914
NVIDIA_DRIVER_CAPABILITIES: compute,utility
LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
PYTORCH_VERSION: v1.12.1-rc5
JOB_USER: training01
JOB_UID: 800001
JOB_GID: 800001
JOB_GROUP: training01
HOME: /home/training01
SHELL: /bin/bash
DATA_DIR: /data/tiny-imagenet-200
NCCL_DEBUG: INFO
OMP_NUM_THREADS: 1
PYTHONUNBUFFERED: 1
PET_NNODES: 1
KUBERNETES_PORT_443_TCP_PORT: 443
KUBERNETES_PORT_443_TCP_ADDR: 172.19.0.1
KUBERNETES_SERVICE_HOST: 172.19.0.1
KUBERNETES_SERVICE_PORT: 443
KUBERNETES_SERVICE_PORT_HTTPS: 443
KUBERNETES_PORT: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP_PROTO: tcp
LC_CTYPE: C.UTF-8
LOCAL_RANK: 2
RANK: 2
GROUP_RANK: 0
ROLE_RANK: 2
ROLE_NAME: default
LOCAL_WORLD_SIZE: 4
WORLD_SIZE: 4
GROUP_WORLD_SIZE: 1
ROLE_WORLD_SIZE: 4
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 29500
TORCHELASTIC_RESTART_COUNT: 0
TORCHELASTIC_MAX_RESTARTS: 0
TORCHELASTIC_RUN_ID: none
TORCHELASTIC_USE_AGENT_STORE: True
NCCL_ASYNC_ERROR_HANDLING: 1
TORCHELASTIC_ERROR_FILE: /tmp/torchelastic_n1gpxvtx/none_6wx_d_96/attempt_0/2/error.json
CRC32C_SW_MODE: auto
PATH: /opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME: dist-worker-0
NVIDIA_VISIBLE_DEVICES: GPU-85172998-46db-3656-f3f1-cb076a965d80,GPU-eebefd56-1ec7-b5f5-bfc5-cf8412b2aae3,GPU-15b5a5c6-cbc7-6aa5-b4d2-4df7e513ba1f,GPU-dd663deb-dee4-f5fb-d516-cd9012bbc914
NVIDIA_DRIVER_CAPABILITIES: compute,utility
LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
PYTORCH_VERSION: v1.12.1-rc5
JOB_USER: training01
JOB_UID: 800001
JOB_GID: 800001
JOB_GROUP: training01
HOME: /home/training01
SHELL: /bin/bash
DATA_DIR: /data/tiny-imagenet-200
NCCL_DEBUG: INFO
OMP_NUM_THREADS: 1
PYTHONUNBUFFERED: 1
PET_NNODES: 1
KUBERNETES_PORT_443_TCP_PORT: 443
KUBERNETES_PORT_443_TCP_ADDR: 172.19.0.1
KUBERNETES_SERVICE_HOST: 172.19.0.1
KUBERNETES_SERVICE_PORT: 443
KUBERNETES_SERVICE_PORT_HTTPS: 443
KUBERNETES_PORT: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP_PROTO: tcp
LC_CTYPE: C.UTF-8
LOCAL_RANK: 3
RANK: 3
GROUP_RANK: 0
ROLE_RANK: 3
ROLE_NAME: default
LOCAL_WORLD_SIZE: 4
WORLD_SIZE: 4
GROUP_WORLD_SIZE: 1
ROLE_WORLD_SIZE: 4
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 29500
TORCHELASTIC_RESTART_COUNT: 0
TORCHELASTIC_MAX_RESTARTS: 0
TORCHELASTIC_RUN_ID: none
TORCHELASTIC_USE_AGENT_STORE: True
NCCL_ASYNC_ERROR_HANDLING: 1
TORCHELASTIC_ERROR_FILE: /tmp/torchelastic_n1gpxvtx/none_6wx_d_96/attempt_0/3/error.json
CRC32C_SW_MODE: auto
PATH: /opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME: dist-worker-0
NVIDIA_VISIBLE_DEVICES: GPU-85172998-46db-3656-f3f1-cb076a965d80,GPU-eebefd56-1ec7-b5f5-bfc5-cf8412b2aae3,GPU-15b5a5c6-cbc7-6aa5-b4d2-4df7e513ba1f,GPU-dd663deb-dee4-f5fb-d516-cd9012bbc914
NVIDIA_DRIVER_CAPABILITIES: compute,utility
LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
PYTORCH_VERSION: v1.12.1-rc5
JOB_USER: training01
JOB_UID: 800001
JOB_GID: 800001
JOB_GROUP: training01
HOME: /home/training01
SHELL: /bin/bash
DATA_DIR: /data/tiny-imagenet-200
NCCL_DEBUG: INFO
OMP_NUM_THREADS: 1
PYTHONUNBUFFERED: 1
PET_NNODES: 1
KUBERNETES_PORT_443_TCP_PORT: 443
KUBERNETES_PORT_443_TCP_ADDR: 172.19.0.1
KUBERNETES_SERVICE_HOST: 172.19.0.1
KUBERNETES_SERVICE_PORT: 443
KUBERNETES_SERVICE_PORT_HTTPS: 443
KUBERNETES_PORT: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP_PROTO: tcp
LC_CTYPE: C.UTF-8
LOCAL_RANK: 1
RANK: 1
GROUP_RANK: 0
ROLE_RANK: 1
ROLE_NAME: default
LOCAL_WORLD_SIZE: 4
WORLD_SIZE: 4
GROUP_WORLD_SIZE: 1
ROLE_WORLD_SIZE: 4
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 29500
TORCHELASTIC_RESTART_COUNT: 0
TORCHELASTIC_MAX_RESTARTS: 0
TORCHELASTIC_RUN_ID: none
TORCHELASTIC_USE_AGENT_STORE: True
NCCL_ASYNC_ERROR_HANDLING: 1
TORCHELASTIC_ERROR_FILE: /tmp/torchelastic_n1gpxvtx/none_6wx_d_96/attempt_0/1/error.json
CRC32C_SW_MODE: auto
PATH: /opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME: dist-worker-0
NVIDIA_VISIBLE_DEVICES: GPU-85172998-46db-3656-f3f1-cb076a965d80,GPU-eebefd56-1ec7-b5f5-bfc5-cf8412b2aae3,GPU-15b5a5c6-cbc7-6aa5-b4d2-4df7e513ba1f,GPU-dd663deb-dee4-f5fb-d516-cd9012bbc914
NVIDIA_DRIVER_CAPABILITIES: compute,utility
LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64
PYTORCH_VERSION: v1.12.1-rc5
JOB_USER: training01
JOB_UID: 800001
JOB_GID: 800001
JOB_GROUP: training01
HOME: /home/training01
SHELL: /bin/bash
DATA_DIR: /data/tiny-imagenet-200
NCCL_DEBUG: INFO
OMP_NUM_THREADS: 1
PYTHONUNBUFFERED: 1
PET_NNODES: 1
KUBERNETES_PORT_443_TCP_PORT: 443
KUBERNETES_PORT_443_TCP_ADDR: 172.19.0.1
KUBERNETES_SERVICE_HOST: 172.19.0.1
KUBERNETES_SERVICE_PORT: 443
KUBERNETES_SERVICE_PORT_HTTPS: 443
KUBERNETES_PORT: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP: tcp://172.19.0.1:443
KUBERNETES_PORT_443_TCP_PROTO: tcp
LC_CTYPE: C.UTF-8
LOCAL_RANK: 0
RANK: 0
GROUP_RANK: 0
ROLE_RANK: 0
ROLE_NAME: default
LOCAL_WORLD_SIZE: 4
WORLD_SIZE: 4
GROUP_WORLD_SIZE: 1
ROLE_WORLD_SIZE: 4
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 29500
TORCHELASTIC_RESTART_COUNT: 0
TORCHELASTIC_MAX_RESTARTS: 0
TORCHELASTIC_RUN_ID: none
TORCHELASTIC_USE_AGENT_STORE: True
NCCL_ASYNC_ERROR_HANDLING: 1
TORCHELASTIC_ERROR_FILE: /tmp/torchelastic_n1gpxvtx/none_6wx_d_96/attempt_0/0/error.json
CRC32C_SW_MODE: auto
dist-worker-0:11:11 [0] NCCL INFO Bootstrap : Using eth0:172.18.1.185<0>
dist-worker-0:11:11 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

dist-worker-0:11:11 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
dist-worker-0:11:11 [0] NCCL INFO NET/Socket : Using [0]eth0:172.18.1.185<0>
dist-worker-0:11:11 [0] NCCL INFO Using network Socket
NCCL version 2.10.3+cuda11.3
dist-worker-0:12:12 [1] NCCL INFO Bootstrap : Using eth0:172.18.1.185<0>
dist-worker-0:13:13 [2] NCCL INFO Bootstrap : Using eth0:172.18.1.185<0>
dist-worker-0:12:12 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
dist-worker-0:13:13 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

dist-worker-0:12:12 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]

dist-worker-0:13:13 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
dist-worker-0:13:13 [2] NCCL INFO NET/Socket : Using [0]eth0:172.18.1.185<0>
dist-worker-0:12:12 [1] NCCL INFO NET/Socket : Using [0]eth0:172.18.1.185<0>
dist-worker-0:13:13 [2] NCCL INFO Using network Socket
dist-worker-0:12:12 [1] NCCL INFO Using network Socket
dist-worker-0:14:14 [3] NCCL INFO Bootstrap : Using eth0:172.18.1.185<0>
dist-worker-0:14:14 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

dist-worker-0:14:14 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
dist-worker-0:14:14 [3] NCCL INFO NET/Socket : Using [0]eth0:172.18.1.185<0>
dist-worker-0:14:14 [3] NCCL INFO Using network Socket
dist-worker-0:14:56 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 0/-1/-1->3->1 [2] 1/-1/-1->3->0 [3] 2/-1/-1->3->-1 [4] -1/-1/-1->3->2 [5] 0/-1/-1->3->1 [6] -1/-1/-1->3->2 [7] 0/-1/-1->3->1 [8] 1/-1/-1->3->0 [9] 2/-1/-1->3->-1 [10] -1/-1/-1->3->2 [11] 0/-1/-1->3->1
dist-worker-0:14:56 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000
dist-worker-0:11:53 [0] NCCL INFO Channel 00/12 :    0   1   2   3
dist-worker-0:13:55 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] -1/-1/-1->2->0 [2] 0/-1/-1->2->-1 [3] 1/-1/-1->2->3 [4] 3/-1/-1->2->1 [5] -1/-1/-1->2->0 [6] 3/-1/-1->2->1 [7] -1/-1/-1->2->0 [8] 0/-1/-1->2->-1 [9] 1/-1/-1->2->3 [10] 3/-1/-1->2->1 [11] -1/-1/-1->2->0
dist-worker-0:12:54 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 3/-1/-1->1->-1 [2] -1/-1/-1->1->3 [3] 0/-1/-1->1->2 [4] 2/-1/-1->1->0 [5] 3/-1/-1->1->-1 [6] 2/-1/-1->1->0 [7] 3/-1/-1->1->-1 [8] -1/-1/-1->1->3 [9] 0/-1/-1->1->2 [10] 2/-1/-1->1->0 [11] 3/-1/-1->1->-1
dist-worker-0:11:53 [0] NCCL INFO Channel 01/12 :    0   1   3   2
dist-worker-0:13:55 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000
dist-worker-0:11:53 [0] NCCL INFO Channel 02/12 :    0   2   3   1
dist-worker-0:12:54 [1] NCCL INFO Setting affinity for GPU 1 to ffff
dist-worker-0:11:53 [0] NCCL INFO Channel 03/12 :    0   2   1   3
dist-worker-0:11:53 [0] NCCL INFO Channel 04/12 :    0   3   1   2
dist-worker-0:11:53 [0] NCCL INFO Channel 05/12 :    0   3   2   1
dist-worker-0:11:53 [0] NCCL INFO Channel 06/12 :    0   1   2   3
dist-worker-0:11:53 [0] NCCL INFO Channel 07/12 :    0   1   3   2
dist-worker-0:11:53 [0] NCCL INFO Channel 08/12 :    0   2   3   1
dist-worker-0:11:53 [0] NCCL INFO Channel 09/12 :    0   2   1   3
dist-worker-0:11:53 [0] NCCL INFO Channel 10/12 :    0   3   1   2
dist-worker-0:11:53 [0] NCCL INFO Channel 11/12 :    0   3   2   1
dist-worker-0:11:53 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 2/-1/-1->0->3 [2] 3/-1/-1->0->2 [3] -1/-1/-1->0->1 [4] 1/-1/-1->0->-1 [5] 2/-1/-1->0->3 [6] 1/-1/-1->0->-1 [7] 2/-1/-1->0->3 [8] 3/-1/-1->0->2 [9] -1/-1/-1->0->1 [10] 1/-1/-1->0->-1 [11] 2/-1/-1->0->3
dist-worker-0:11:53 [0] NCCL INFO Setting affinity for GPU 0 to ffff
dist-worker-0:14:56 [3] NCCL INFO Channel 00 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 00 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 00 : 1[62000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 03 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 02 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 04 : 1[62000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 06 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 06 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 06 : 1[62000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 09 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 08 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 10 : 1[62000] -> 2[89000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 00 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 01 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 06 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 07 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 01 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 04 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 07 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 10 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 02 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 01 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 02 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 04 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 03 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 03 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 08 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 08 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 07 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 10 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 09 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 09 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 03 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 01 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 04 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 05 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 05 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 05 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 09 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 07 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 10 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 11 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 11 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 11 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 02 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 05 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 08 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 11 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Connected all rings
dist-worker-0:13:55 [2] NCCL INFO Connected all rings
dist-worker-0:11:53 [0] NCCL INFO Connected all rings
dist-worker-0:12:54 [1] NCCL INFO Connected all rings
dist-worker-0:14:56 [3] NCCL INFO Channel 01 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 03 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 02 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 04 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 05 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 07 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 09 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 03 : 1[62000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 08 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 10 : 2[89000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 09 : 1[62000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 11 : 3[8a000] -> 0[61000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 03 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 04 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 09 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 10 : 0[61000] -> 1[62000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 02 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 05 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 08 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 11 : 2[89000] -> 0[61000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 02 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 05 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 08 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 01 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 01 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 11 : 1[62000] -> 3[8a000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 05 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 05 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 07 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 07 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 11 : 0[61000] -> 2[89000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 11 : 3[8a000] -> 1[62000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 00 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 00 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 03 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 03 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 00 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 04 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 04 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 06 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 04 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 06 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 09 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 06 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 09 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:12:54 [1] NCCL INFO Channel 10 : 1[62000] -> 0[61000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Channel 10 : 2[89000] -> 1[62000] via P2P/IPC
dist-worker-0:14:56 [3] NCCL INFO Channel 10 : 3[8a000] -> 2[89000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 01 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 02 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 07 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:11:53 [0] NCCL INFO Channel 08 : 0[61000] -> 3[8a000] via P2P/IPC
dist-worker-0:13:55 [2] NCCL INFO Connected all trees
dist-worker-0:13:55 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
dist-worker-0:13:55 [2] NCCL INFO 12 coll channels, 16 p2p channels, 4 p2p channels per peer
dist-worker-0:12:54 [1] NCCL INFO Connected all trees
dist-worker-0:12:54 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
dist-worker-0:14:56 [3] NCCL INFO Connected all trees
dist-worker-0:14:56 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
dist-worker-0:11:53 [0] NCCL INFO Connected all trees
dist-worker-0:11:53 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
dist-worker-0:12:54 [1] NCCL INFO 12 coll channels, 16 p2p channels, 4 p2p channels per peer
dist-worker-0:14:56 [3] NCCL INFO 12 coll channels, 16 p2p channels, 4 p2p channels per peer
dist-worker-0:11:53 [0] NCCL INFO 12 coll channels, 16 p2p channels, 4 p2p channels per peer
dist-worker-0:12:54 [1] NCCL INFO comm 0x7fe2f8003090 rank 1 nranks 4 cudaDev 1 busId 62000 - Init COMPLETE
dist-worker-0:14:56 [3] NCCL INFO comm 0x7f3650003090 rank 3 nranks 4 cudaDev 3 busId 8a000 - Init COMPLETE
dist-worker-0:11:53 [0] NCCL INFO comm 0x7f47f4003090 rank 0 nranks 4 cudaDev 0 busId 61000 - Init COMPLETE
dist-worker-0:13:55 [2] NCCL INFO comm 0x7efb60003090 rank 2 nranks 4 cudaDev 2 busId 89000 - Init COMPLETE
dist-worker-0:11:11 [0] NCCL INFO Launch mode Parallel
/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:281: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Starting training on GPU 2 of 4 -- 
2: Entering training loop for epoch 0
/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:281: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Starting training on GPU 1 of 4 -- 
1: Entering training loop for epoch 0
Starting training on GPU 0 of 4 -- 
/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:281: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
0: Entering training loop for epoch 0
/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:281: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Starting training on GPU 3 of 4 -- 
3: Entering training loop for epoch 0
2: Entering validation for epoch 0
0: Entering validation for epoch 0
1: Entering validation for epoch 0
3: Entering validation for epoch 0
0: Collecting metric for epoch 0
3: Collecting metric for epoch 0
1: Collecting metric for epoch 0
1: Entering training loop for epoch 1
2: Collecting metric for epoch 0
0: Writing metric for epoch 0
[1] :Loss (train, val):0.199, 1.310| Accuracy (train,val): 0.295, 0.000
2: Entering training loop for epoch 1
3: Entering training loop for epoch 1
0: Entering training loop for epoch 1
1: Entering validation for epoch 1
3: Entering validation for epoch 1
2: Entering validation for epoch 1
0: Entering validation for epoch 1
2: Collecting metric for epoch 1
1: Collecting metric for epoch 1
1: Entering training loop for epoch 2
2: Entering training loop for epoch 2
3: Collecting metric for epoch 1
3: Entering training loop for epoch 2
0: Collecting metric for epoch 1
0: Writing metric for epoch 1
[2] :Loss (train, val):0.173, 0.949| Accuracy (train,val): 0.000, 0.000
0: Entering training loop for epoch 2
3: Entering validation for epoch 2
2: Entering validation for epoch 2
0: Entering validation for epoch 2
1: Entering validation for epoch 2
0: Collecting metric for epoch 2
1: Collecting metric for epoch 2
1: Entering training loop for epoch 3
2: Collecting metric for epoch 2
2: Entering training loop for epoch 3
3: Collecting metric for epoch 2
0: Writing metric for epoch 2
[3] :Loss (train, val):0.170, 0.835| Accuracy (train,val): 0.000, 0.000
3: Entering training loop for epoch 3
0: Entering training loop for epoch 3
3: Entering validation for epoch 3
1: Entering validation for epoch 3
0: Entering validation for epoch 3
2: Entering validation for epoch 3
0: Collecting metric for epoch 3
1: Collecting metric for epoch 3
2: Collecting metric for epoch 3
3: Collecting metric for epoch 3
0: Writing metric for epoch 3
[4] :Loss (train, val):0.169, 1.035| Accuracy (train,val): 0.003, 0.000

Finished Training
Total execution time = 418.770 sec
Max memory used by tensors = 3186634752 bytes
