{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b2e43b-a55c-422e-8c28-e1c4144c9a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -q kfp==2.0.0b12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9b03c6-77e2-4363-9480-33917ded59fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a4da74-7447-4867-9d7a-b6a084344cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import OutputPath,InputPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8dd5f0-6f3e-4282-902f-4f09baac08d4",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93928db-248c-46d7-9f8c-273c8430d1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_data(download_link: str, data_path: OutputPath(str)):\n",
    "    import zipfile\n",
    "    import wget\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # download files\n",
    "    wget.download(download_link.format(file='train'), f'{data_path}/train_csv.zip')\n",
    "    wget.download(download_link.format(file='test'), f'{data_path}/test_csv.zip')\n",
    "    \n",
    "    with zipfile.ZipFile(f\"{data_path}/train_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "        \n",
    "    with zipfile.ZipFile(f\"{data_path}/test_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc96cb3-8850-40aa-8932-b54a6a998185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_op = dsl.component(func=download_data,\n",
    "                            base_image=\"python:3.7\", \n",
    "                            packages_to_install=['wget', 'zipfile36'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375c642-02cc-4f67-a5ba-7026486e2f0a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7c01f6-7869-488d-b43e-1ecda3087c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(data_path: InputPath(str),load_data_path:  OutputPath(str)):\n",
    "    import pandas as pd\n",
    "    import os, pickle\n",
    "    \n",
    "    train_data_path = data_path + '/train.csv'\n",
    "    test_data_path = data_path + '/test.csv'\n",
    "    tweet_df= pd.read_csv(train_data_path)\n",
    "    test_df=pd.read_csv(test_data_path)\n",
    "    df=pd.concat([tweet_df,test_df])\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(load_data_path, exist_ok = True)\n",
    "    \n",
    "    # join train and test together\n",
    "    ntrain = tweet_df.shape[0]\n",
    "    ntest = test_df.shape[0]\n",
    "    with open(f'{load_data_path}/df', 'wb') as f:\n",
    "        pickle.dump((ntrain, df, tweet_df), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f1ae34-f823-401d-982b-9b46e7b6a0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_data_step = dsl.component(func=load_data,\n",
    "                               base_image='python:3.7',\n",
    "                               packages_to_install=['pandas','pickle5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edd204-cec5-448d-9109-d9a5fc3de25d",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61e3f12-40f0-4809-9e7d-168fa38fc52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(load_data_path: InputPath(str), preprocess_data_path: OutputPath(str)):\n",
    "    \n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import os, pickle\n",
    "    import string\n",
    "    \n",
    "     #loading the train data\n",
    "    with open(f'{load_data_path}/df', 'rb') as f:\n",
    "        ntrain, df, tweet_df = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    def remove_URL(text):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url.sub(r'',text)\n",
    "    def remove_html(text):\n",
    "        html=re.compile(r'<.*?>')\n",
    "        return html.sub(r'',text)\n",
    "    def remove_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    def remove_punct(text):\n",
    "        table=str.maketrans('','',string.punctuation)\n",
    "        return text.translate(table)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    df['text'] = df['text'].apply(lambda x : remove_URL(x))\n",
    "    df['text'] = df['text'].apply(lambda x: remove_html(x))\n",
    "    df['text'] = df['text'].apply(lambda x: remove_emoji(x))\n",
    "    df['text'] = df['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(preprocess_data_path, exist_ok = True)\n",
    "    \n",
    "    with open(f'{preprocess_data_path}/df', 'wb') as f:\n",
    "        pickle.dump((ntrain, df, tweet_df), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e6d8cc-b32a-469b-b2ad-7f398f4d8204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_data_step = dsl.component(func=preprocess_data,\n",
    "                                     packages_to_install=['pandas', 'regex', 'pickle5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a84581-8079-4be5-9ae7-f0a335002ca5",
   "metadata": {},
   "source": [
    "# Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c4fa12-0dd7-49e4-9d06-aabf6a7925ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def corpus_creation(preprocess_data_path: InputPath(str), corpus_path: OutputPath(str)):\n",
    "    import os, pickle\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.util import ngrams\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    stop=set(stopwords.words('english'))\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open(f'{preprocess_data_path}/df', 'rb') as f:\n",
    "        ntrain, df, tweet_df = pickle.load(f)\n",
    "        \n",
    "    def create_corpus(df):\n",
    "        corpus=[]\n",
    "        for tweet in tqdm(df['text']):\n",
    "            words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "            corpus.append(words)\n",
    "        return corpus\n",
    "    \n",
    "     #creating the preprocess directory\n",
    "    os.makedirs(corpus_path, exist_ok = True)\n",
    "    \n",
    "    corpus=create_corpus(df)\n",
    "    with open(f'{corpus_path}/corpus', 'wb') as f:\n",
    "        pickle.dump((corpus,tweet_df), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f004c5-65e5-4dc4-8511-a846ca6e16cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus_creation_step = dsl.component(func=corpus_creation,\n",
    "                                     packages_to_install=['pandas', 'pickle5', 'nltk','tqdm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117101b-6a80-40e6-9448-e7b6dee41338",
   "metadata": {},
   "source": [
    "# Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f4a5db-1d7c-4acf-b2f8-e29b488f3b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embedding_step(download_link_glove: str,\n",
    "                   corpus_path: InputPath(str), \n",
    "                   embedded_path: OutputPath(str)):\n",
    "    \n",
    "    import os, pickle\n",
    "    import pandas as pd\n",
    "    import zipfile\n",
    "    import wget\n",
    "    import os\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.utils import pad_sequences\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open(f'{corpus_path}/corpus', 'rb') as f:\n",
    "        corpus, tweet_df = pickle.load(f)\n",
    "    \n",
    "    if not os.path.exists(embedded_path):\n",
    "        os.makedirs(embedded_path)\n",
    "    # download files\n",
    "    wget.download(download_link_glove, f'{embedded_path}/glove.6B.zip')\n",
    "    \n",
    "    with zipfile.ZipFile(f'{embedded_path}/glove.6B.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(embedded_path)\n",
    "    \n",
    "    embedding_dict={}\n",
    "    \"\"\"path_to_glove_file = os.path.join(\n",
    "        os.path.expanduser(\"~\"), f\"{embedded_path}/glove.6B.100d.txt\"\n",
    "    )\"\"\"\n",
    "    with open(f\"{embedded_path}/glove.6B.100d.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            vectors=np.asarray(values[1:],'float32')\n",
    "            embedding_dict[word]=vectors\n",
    "    f.close()\n",
    "    \n",
    "    MAX_LEN=50\n",
    "    tokenizer_obj=Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(corpus)\n",
    "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "    \n",
    "    tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "    word_index=tokenizer_obj.word_index\n",
    "    num_words=len(word_index)+1\n",
    "    embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "    for word,i in tqdm(word_index.items()):\n",
    "        if i > num_words:\n",
    "            continue\n",
    "\n",
    "        emb_vec=embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec\n",
    "    \n",
    "    with open(f'{embedded_path}/embedding', 'wb') as f:\n",
    "        pickle.dump((embedding_matrix, num_words, tweet_pad, tweet_df, MAX_LEN), f)\n",
    "    return(print('Done!'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569a3440-e5c7-4b98-b602-42501b343e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_creation_step = dsl.component(embedding_step, \n",
    "                                        packages_to_install=['pandas', 'zipfile36', 'wget','tqdm','keras','numpy','tensorflow', 'pickle5'],\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749b8f2-e75c-4540-9328-66081a2e5ba0",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e60dcc9-c58c-4556-8b92-9a947dfa10db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_building_and_training(embedded_path: InputPath(str), model_path: OutputPath(str)):\n",
    "    \n",
    "    import os, pickle;\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "    from keras.initializers import Constant\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    \n",
    "    with open(f'{embedded_path}/embedding', 'rb') as f:\n",
    "        embedding_matrix, num_words, tweet_pad, tweet_df, MAX_LEN = pickle.load(f)\n",
    "    \n",
    "    train=tweet_pad[:tweet_df.shape[0]]\n",
    "    final_test=tweet_pad[tweet_df.shape[0]:]\n",
    "    X_train,X_test,y_train,y_test=train_test_split(train,tweet_df['target'].values,test_size=0.15)\n",
    "    \n",
    "    #defining model\n",
    "    model=Sequential()\n",
    "\n",
    "    embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                       input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "    model.add(embedding)\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    optimzer=Adam(learning_rate=1e-5)\n",
    "    \n",
    "    #Compiling the classifier model with Adam optimizer\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n",
    "    \n",
    "    #fitting the model\n",
    "    history=model.fit(X_train,y_train,batch_size=4,epochs=5,validation_data=(X_test,y_test),verbose=2)\n",
    "\n",
    "    #evaluate model\n",
    "    test_loss, test_acc = model.evaluate(np.array(X_test),  np.array(y_test), verbose=0)\n",
    "    print(\"Test_loss: {}, Test_accuracy: {} \".format(test_loss,test_acc))\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(model_path, exist_ok = True)\n",
    "    \n",
    "    #saving the model\n",
    "    model.save(f'{model_path}/model.h5')\n",
    "    \n",
    "    #dumping other values\n",
    "    with open(f'{model_path}/values', 'wb') as f:\n",
    "        pickle.dump((test_loss, test_acc), f)\n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e63eb10-b0c5-4414-8634-1c2066db2c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_building_step = dsl.component(model_building_and_training, \n",
    "                                    packages_to_install=['pandas', 'zipfile36', 'wget','tqdm','keras','numpy','tensorflow','sklearn','pickle5'], \n",
    "                                    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aebe46f0-b825-4c8a-8280-d8137d768c2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "@dsl.pipeline(\n",
    "   name='nlp-pipeline',\n",
    "   description='An example of NLP tasks from data prep to model training.'\n",
    ")\n",
    "def trial_pipeline(\n",
    "    download_link: str = \"https://github.com/AnkitRai-22/natural-language-processing-with-disaster-tweets-kaggle-competition/blob/main/data/{file}.csv.zip?raw=true\",\n",
    "    data_path: str = \"/mnt\",\n",
    "    load_data_path: str = \"load\", \n",
    "    preprocess_data_path: str = \"preprocess\",\n",
    "    corpus_path:str = \"corpus\" ,\n",
    "    download_link_glove:str = \"http://nlp.stanford.edu/data/glove.6B.zip\" ,\n",
    "    model_path:str=\"model\",\n",
    "):\n",
    "    download_container = download_op(download_link=download_link)\n",
    "    output1 = load_data_step(data_path=download_container.output)\n",
    "    output2 = preprocess_data_step(load_data_path=output1.output)\n",
    "    output3 = corpus_creation_step(preprocess_data_path=output2.output)\n",
    "    output4 = embedding_creation_step(download_link_glove=download_link_glove, corpus_path=output3.output)\n",
    "    output5 = model_building_step(embedded_path=output4.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "faa60ca3-4ce7-482d-a51a-1b223fedf802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='nlp-pipeline',\n",
    "   description='An example of NLP tasks from data prep to model training.'\n",
    ")\n",
    "def trial_pipeline(\n",
    "    download_link: str,\n",
    "    data_path: str ,\n",
    "    load_data_path: str, \n",
    "    preprocess_data_path: str ,\n",
    "    corpus_path:str ,\n",
    "    download_link_glove:str  ,\n",
    "    model_path:str,\n",
    "):\n",
    "    download_container = download_op(download_link=download_link)\n",
    "    output1 = load_data_step(data_path=download_container.output)\n",
    "    output2 = preprocess_data_step(load_data_path=output1.output)\n",
    "    output3 = corpus_creation_step(preprocess_data_path=output2.output)\n",
    "    output4 = embedding_creation_step(download_link_glove=download_link_glove, corpus_path=output3.output)\n",
    "    \n",
    "    download_container.set_gpu_limit='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57bb6906-844e-45bd-ae68-1f60eaebcf40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=trial_pipeline,package_path='nlp_pipeline01.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fabeb337-566d-4a55-b64f-0fadba999e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.trial_pipeline(download_link: str, data_path: str, load_data_path: str, preprocess_data_path: str, corpus_path: str, download_link_glove: str, model_path: str)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6ed5b15-6224-48aa-b6e0-d50f3ebc0897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function component in module kfp.components.component_decorator:\n",
      "\n",
      "component(func: Optional[Callable] = None, *, base_image: Optional[str] = None, target_image: Optional[str] = None, packages_to_install: List[str] = None, pip_index_urls: Optional[List[str]] = None, output_component_file: Optional[str] = None, install_kfp_package: bool = True, kfp_package_path: Optional[str] = None)\n",
      "    Decorator for Python-function based components.\n",
      "    \n",
      "    A KFP component can either be a lightweight component or a containerized\n",
      "    component.\n",
      "    \n",
      "    If ``target_image`` is not specified, this function creates a lightweight\n",
      "    component. A lightweight component is a self-contained Python function that\n",
      "    includes all necessary imports and dependencies. In lightweight components,\n",
      "    ``packages_to_install`` will be used to install dependencies at runtime. The\n",
      "    parameters ``install_kfp_package`` and ``kfp_package_path`` can be used to control\n",
      "    how and from where KFP should be installed when the lightweight component is executed.\n",
      "    \n",
      "    If ``target_image`` is specified, this function creates a component definition\n",
      "    based around the ``target_image``. The assumption is that the function in ``func``\n",
      "    will be packaged by KFP into this ``target_image``. You can use the KFP CLI's ``build``\n",
      "    command to package the function into ``target_image``.\n",
      "    \n",
      "    Args:\n",
      "        func: Python function from which to create a component. The function\n",
      "            should have type annotations for all its arguments, indicating how\n",
      "            each argument is intended to be used (e.g. as an input/output artifact,\n",
      "            a plain parameter, or a path to a file).\n",
      "        base_image: Image to use when executing the Python function. It should\n",
      "            contain a default Python interpreter that is compatible with KFP.\n",
      "        target_image: Image to when creating containerized components.\n",
      "        packages_to_install: List of packages to install before\n",
      "            executing the Python function. These will always be installed at component runtime.\n",
      "        pip_index_urls: Python Package Index base URLs from which to\n",
      "            install ``packages_to_install``. Defaults to installing from only PyPI\n",
      "            (``'https://pypi.org/simple'``). For more information, see `pip install docs <https://pip.pypa.io/en/stable/cli/pip_install/#cmdoption-0>`_.\n",
      "        output_component_file: If specified, this function will write a\n",
      "            shareable/loadable version of the component spec into this file.\n",
      "    \n",
      "            **Warning:** This compilation approach is deprecated.\n",
      "        install_kfp_package: Specifies if the KFP SDK should add the ``kfp`` Python package to\n",
      "            ``packages_to_install``. Lightweight Python functions always require\n",
      "            an installation of KFP in ``base_image`` to work. If you specify\n",
      "            a ``base_image`` that already contains KFP, you can set this to ``False``.\n",
      "            This flag is ignored when ``target_image`` is specified, which implies\n",
      "            a choice to build a containerized component. Containerized components\n",
      "            will always install KFP as part of the build process.\n",
      "        kfp_package_path: Specifies the location from which to install KFP. By\n",
      "            default, this will try to install from PyPI using the same version\n",
      "            as that used when this component was created. Component authors can\n",
      "            choose to override this to point to a GitHub pull request or\n",
      "            other pip-compatible package server.\n",
      "    \n",
      "    Returns:\n",
      "        A component task factory that can be used in pipeline definitions.\n",
      "    \n",
      "    Example:\n",
      "      ::\n",
      "    \n",
      "        from kfp import dsl\n",
      "    \n",
      "        @dsl.component\n",
      "        def my_function_one(input: str, output: Output[Model]):\n",
      "            ...\n",
      "    \n",
      "        @dsl.component(\n",
      "        base_image='python:3.9',\n",
      "        output_component_file='my_function.yaml'\n",
      "        )\n",
      "        def my_function_two(input: Input[Mode])):\n",
      "            ...\n",
      "    \n",
      "        @dsl.pipeline(name='my-pipeline', pipeline_root='...')\n",
      "        def pipeline():\n",
      "            my_function_one_task = my_function_one(input=...)\n",
      "            my_function_two_task = my_function_two(input=my_function_one_task.outputs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dsl.component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6a219-a3e1-46e9-ad9e-022e9631cdd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
