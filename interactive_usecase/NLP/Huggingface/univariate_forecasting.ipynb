{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7248dd-daeb-46db-9d84-5392029a7b60",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "\n",
    "This material is adapted from the blog post from  [Hugging Face on Probablistic Forecasting](https://huggingface.co/blog/time-series-transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31480a-a0b1-4c9c-a385-d461464edce8",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff825a6-4706-4a62-b15a-6047368d518a",
   "metadata": {},
   "source": [
    "Use vanilla transformer from Vaswani et al, 2017) for a univariate probablistic forecasting task. This mean predicting 1-d time serise dirstribution (if there are more than one, then one at a time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4cf0a-c780-423e-9874-da72a8f9efee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Why a Transformer??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f6220-b9d6-4890-8875-a3cef7d16d7d",
   "metadata": {},
   "source": [
    "Encoder-Decoder transformer is a intuitive choice becasue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd89be-d495-4304-8281-86ac95a1f80e",
   "metadata": {},
   "source": [
    "- The Encoder-Decoder architecture maps well to the requirement of forecasting some prediction steps in future. This is analogus to the text generation task. \n",
    "    - Remember in text generation, decoder's output, a single token, is passed back into decoder (appended to the previous input sequence as the last token) and fed back to the decoder -- autoregressive generation\n",
    "    - Because of the above, such model can generate outputs with longer sequence lengths than the input\n",
    "    - As a naive approach Greedy sampling/search -- meaning that output with highest probabilty among the possible outputs will be chosen and be considered for autoregressively fed back to the decoder (other choices, e.g. Beam search for better memory utilization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38e99c-b847-4e3f-8ec0-44dac8a7d19b",
   "metadata": {},
   "source": [
    "- A transformer is a suitable choice for very long sequence lenghts. Time series can have long context lengths and fitting very large dataset into a single context may be prohibitive. With a transformer, we can choose an appropriate context window within the input batches and use SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c4c74a-3b06-462b-9ace-d2b393e3726d",
   "metadata": {},
   "source": [
    "- A transfomer can also help  the missing data points in the training data. Managing it is similiar to enforcing `attention_mask` when feeding the next tokens to decode and as one would manage the padding tokens in the input sequence, when training BERT or GPT2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1730217-3b6e-4335-837f-cacf64c1fa97",
   "metadata": {},
   "source": [
    "# Hands on action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88365f12-4f05-4e44-8343-1b2e1d87d71e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706a228-4111-461a-93d6-508566393c39",
   "metadata": {},
   "source": [
    "We are using `tourism_monthly` dataset as an example in today's exercise. \n",
    "\n",
    "- The dataset is composed of monthly tourism volumes for 366 regions in Australia\n",
    "- This dataset is available on [Hugging Face Hub](https://huggingface.co/datasets/monash_tsf)\n",
    "- The dataset is part of [Monash Time Series Forecasting](https://forecastingdata.org/) repository, a collection of datasets from different domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d062bda-ce96-4f84-a5d3-f3f96970304f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"monash_tsf\",\"tourism_monthly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7864fc-1bb2-4d44-8cf9-c4a7f566b786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b6267f-7fcb-4336-ba1d-4c04e9ce8946",
   "metadata": {},
   "source": [
    "- Dataset is split in `train`, `test` and `validation` sets\n",
    "- Each of the three datasets are dictionaries. Let's explore the values of `start` and `target` keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1bd122-44fe-4180-935d-f9e61da4aba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_example = dataset['train'][0]\n",
    "train_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5cc2b-960f-4d0f-9d42-dd4aab6f041b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_example['start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8bc8c-9cad-4d2c-8aa8-a4f4351995bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `start` indicates the start of the time series and `target` contains the actual values of the time series.\n",
    "\n",
    "The `start` will be used to add the time related features to the time series values, as extra input to the model. Since this is `monthly` data, we know that each element of the values array corresponds to a month's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67537ce2-acef-49a6-b2c2-f1ee5314befb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_example['target'])\n",
    "print(len(train_example['target']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510aeba-3635-4cd8-ac1e-23a579131e2f",
   "metadata": {},
   "source": [
    "The entries in the validation set is `prediction_length` longer than the training set. `prediction_lenght` is the amount of time point in future the model is expected to forecast. \n",
    "\n",
    "Since the validation will be used to compare with the forecasted values\n",
    "Test set is one or times 'prediction_lenght' longer than the training set. \n",
    "In our case the `prediction_length` is 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7c936-c23a-4cbb-b212-18387436cd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_example = dataset[\"validation\"][0]\n",
    "validation_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3dde7-6d8f-46a2-9c05-96cdb070bf9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_length = len(validation_example['target'])-len(train_example['target'])\n",
    "print(prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50764421-907e-41a0-80bb-730be2f2ba45",
   "metadata": {},
   "source": [
    "Let's check how our data looks. The plot below shows training samples appended by the validation samples of length `predicted_length`. After the training, our model should be tracking the \"expected\" forecast in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858011f1-b46c-404b-b1f4-b73afb6b2402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure, axes = plt.subplots()\n",
    "axes.plot(train_example[\"target\"], color=\"blue\")\n",
    "axes.plot(validation_example[\"target\"], color=\"red\", alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8f0d8-000b-40ab-9f37-45767318b1b7",
   "metadata": {},
   "source": [
    "Let's prepare our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41b025-0b38-4e53-b6ee-757539ee7217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset  = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7e3ff-00a8-46c0-92a6-a55769ac6444",
   "metadata": {},
   "source": [
    "### Add `start` feature to `pd.Period`\n",
    "First we convert the `start` dates to pandas `Period` index using the frequency of the data. For time series data, `pandas.Period` is a class to add metadata to the series with information of its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23de66e-8a6e-41ed-a065-7f6948d8295c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "freq='1M'\n",
    "\n",
    "# lru_cache is a convinient decorator to \n",
    "# cache preceding function Least Recent Cache\n",
    "@lru_cache(10_000)\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq)\n",
    "\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd831aa-cc2e-4137-9b3d-472376878806",
   "metadata": {},
   "source": [
    "Let's add this to our transformation in our dataloading process.\n",
    "\n",
    "Run the cell below to see the before and after result (first 5 elements) of this transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7eb554-8f3f-494d-b9f2-6cb74eee130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['start'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328de81-b45b-4405-bb81-b46a0326894c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# parital functions allow creating new functions \n",
    "# from pre-defined function with different number of required args  \n",
    "\n",
    "train_dataset.set_transform(partial(transform_start_field,freq=freq))\n",
    "test_dataset.set_transform(partial(transform_start_field,freq=freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f36706-9e83-4990-ac35-f7ab43daa2bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset['start'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fd1c2-994f-4b10-920e-a39990cf14ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the Model\n",
    "\n",
    "We will train a model from scratch. \n",
    "\n",
    "To initialize the model randomly, we will modify the `config` parameters. \n",
    "\n",
    "Since we know our dataset, we can add or extra features depending on our knowledge, to help the model. The following additional features are configurable and will be added to our model as parameters.\n",
    "\n",
    "- `prediction_length` (in our case, `24` months): this is the horizon that the decoder of the Transformer will learn to predict for;\n",
    "- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if none is specified;\n",
    "- `lags` for a given frequency: these specify how much we \"look back\" at the past time points. Thus our actual sequence lenght will be `context_length + lags`. This is to be added as additional features. e.g. for a `Daily` frequency `[1, 2, 7, 30, ...]` or in other words look back 1, 2, ... days while for `Minute` data `[1, 30, 60, 60*24, ...]` etc.;\n",
    "- the number of **time features**: in our case, this will be `2` as we'll add `MonthOfYear` and `Age` features;\n",
    "- the number of static categorical features: in our case, this will be just `1` as we'll add a single \"time series ID\" feature;\n",
    "- the `cardinality` is the number of values of each static categorical feature, as a list which for our case will be `[366]` as we have 366 different time series\n",
    "- the `embedding dimension`: the embedding dimension for each static categorical feature, as a list, for example `[3]` meaning the model will learn an embedding vector of size `3` for each of the `366` time series (regions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aefe3d-5fba-480b-a096-d0b4d8bd588c",
   "metadata": {},
   "source": [
    "Let's use the default lags provided by GluonTS for the given frequency (\"monthly\")\n",
    "\n",
    "** [GluonTS](https://ts.gluon.ai/stable/) is a Python package for probabilistic time series modeling, focusing on deep learning based models, based on PyTorch and MXNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23dd32-bf34-4a68-8b51-8f7645390336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "print(lags_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e41a3-a42f-4a54-b57b-24094d5ee018",
   "metadata": {},
   "source": [
    "`lags_sequence` implies that the model will look-back upto 37 months for each time step, as additional features. \n",
    "\n",
    "We will also use the default time features defined in GluonTS (returns a `list`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e3f8f-226c-4915-ab11-aaf795b0ba5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "print(time_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c19bdc-35fe-457b-90ef-e03c6876cf54",
   "metadata": {},
   "source": [
    "In our case, we have only one time feature, month of year.\n",
    "This means, for each time step, the timestamps will be encoded as e.g. 1 - January, 2-February, ... etc)\n",
    "\n",
    "Let's now define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9d668-8f4d-48c7-8374-311cd34686fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags coming from helper given the freq:\n",
    "    lags_sequence=lags_sequence,\n",
    "    # we'll add 2 time features (\"month of year\" and \"age\", see further):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    # we have a single static categorical feature, namely time series ID:\n",
    "    num_static_categorical_features=1,\n",
    "    # it has 366 possible values:\n",
    "    cardinality=[len(train_dataset)],\n",
    "    # the model will learn an embedding of size 2 for each of the 366 possible values:\n",
    "    embedding_dimension=[2],\n",
    "\n",
    "    # transformer params:\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32,\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c356257-ff1e-4d63-8447-9860eefcdade",
   "metadata": {},
   "source": [
    "Hugging Face transformers come as modular blocks with a base or headless model architecture and can support predefined heads on top to do different tasks (as long as the base model supports that task). \n",
    "\n",
    "Here we use Hugging Face's `TimeSeriesTransformerModel` as base or raw model and for forecasting, a distribution head named `TimeSeriesTransformerForPrediction`. The output of the distribtuion head. \n",
    "\n",
    "The distribution head uses Student-t distribution in the Encoder for building self-attention ([check here for possible options](https://huggingface.co/docs/transformers/v4.38.1/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel)).\n",
    "\n",
    "**This is different from Transformers in NLP where the attention head typically consists of a fixed categorical distribution implemented as an `nn.Linear` layer**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78526080-024b-4e88-8952-d9804749ab8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c2e3a-cb95-4984-9f15-8c53723e48ba",
   "metadata": {},
   "source": [
    "Similar to transforms on images using `torchvision.transforms.Compose` here we use GluonTS to `Chain` some transformations. \n",
    "\n",
    "Some of these transformations will add the time features to the data, e.g. month of the year, as an index, as previously discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8093b-ed7b-4768-b723-05f671816a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c31c1-1234-4455-a9b7-9bde1f0d78c5",
   "metadata": {},
   "source": [
    "Below is an example of a set of transformations we seek to be applied to the dataset before the input batch is dispatched to the model. \n",
    "\n",
    "These transformations may include:\n",
    "- removal of certain fields\n",
    "- addition of static categorical features (FEAT_STATIC_CAT) -- features which donot change over the all timestamps, e.g. time series ID, patient's ID etc\n",
    "- addition of static real-valued features (FEAT_STATIC_REAL) -- e.g. price of an item in the time series tracking its sale, float16 or 32\n",
    "- addition of Age feature: an monotonically increasing counter, which keeps track of \"at which point in life\" a time series is. Age feature helps in positional encoding of the inputs, and unlike in NLP transformers where positional encodings are learnt, these are provided as an additional feature of time series. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b96180-c4f2-4923-935b-1160b230c01b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "# returning transformation, the function below selects feartures to remove\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    remove_field_names = []\n",
    "    \n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "        \n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the Time...ForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "            # step 5: add another temporal feature (just a single number)\n",
    "            # tells the model where in the life the value of the time series is\n",
    "            # sort of running counter\n",
    "            AddAgeFeature(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_AGE,\n",
    "                pred_length=config.prediction_length,\n",
    "                log_scale=True,\n",
    "            ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14f68a-97fa-42b1-b7ad-ece9475f1e54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define `InstanceSplitter`\n",
    "**the window sampler**\n",
    "\n",
    "An `InstanceSplitter` samples the windows from the dataset. The splitter randomly samples time series  `context_lenght` and `prediction_length` sized chunks and constructs a window. \n",
    "\n",
    "*`context_lenght` is the input length to encoder whereas `prediction_length` is the size or length of future horizon or number of predicted points in future* \n",
    "\n",
    "\n",
    "The `InstanceSplitter` can be configured into 3 modes:\n",
    "1. `mode=\"train\"`: Here we sample the context and prediction length windows randomly from the dataset given to it (the training dataset). Both past and future values are included in these samples\n",
    "2. `mode=\"validation\"`: Here we sample the very last context length window and prediction window from the dataset given to it (for the back-testing or validation likelihood calculations)\n",
    "3. `mode=\"test\"`: Here we sample the very last context length window only (for the prediction use case)\n",
    "\n",
    "\n",
    "*`ExpectedNumInstanceSampler`: Keeps track of the average time series length and adjusts the probability per time point such that on average `num_instances` training examples are generated per time series* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e84b9a-8b68-4f52-8448-ecef755f7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa8e26-b32f-4fce-a918-13a1f55fe3e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Dataloaders\n",
    "Now its time to create DataLoaders to:\n",
    "- ingest data from disk\n",
    "- apply transformations \n",
    "- create batches (`input`:`output` pairs -- in our case past and future values)\n",
    "- returns an iterator\n",
    "\n",
    "*`Cached`: a utility wrapper to keep reusable elements of sequences in a list to avoid unnecessary recomputation of deterministic iteratable*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180a2e4-95b1-483c-946b-05f41a72829a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from gluonts.itertools import Cyclic, Cached\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    cache_data: bool = True,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    \n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "    if cache_data:\n",
    "        transformed_data = Cached(transformed_data)\n",
    "\n",
    "    # we initialize a Training instance \n",
    "    instance_splitter = create_instance_splitter(config, \"train\")\n",
    "\n",
    "    # the instance splitter will sample a window of size\n",
    "    # context length + lags + prediction length \n",
    "    # (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    stream = Cyclic(transformed_data).stream()\n",
    "    training_instances = instance_splitter.apply(stream)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        training_instances,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer_length=shuffle_buffer_length,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6292f033-6ec6-40bf-80fe-de29220c4f87",
   "metadata": {},
   "source": [
    "Remember from above, the validation set contains the same data as the training set, + `prediction_length` time points more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a120ec-d888-40b0-a332-b4800d99023b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_backtest_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data)\n",
    "\n",
    "    # We create a Validation Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"validation\")\n",
    "\n",
    "    # we apply the transformations in train mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=True)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        testing_instances,\n",
    "        batch_size=batch_size,\n",
    "        output_type=torch.tensor,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155833a-a4c5-4895-8b32-ac60161c4ac8",
   "metadata": {},
   "source": [
    "Let's instantiate the dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d83466-21b2-4914-93ea-4d1c4abb870f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=256,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = create_backtest_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=test_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a2509-890c-42a5-953b-54e1105088a7",
   "metadata": {},
   "source": [
    "Let's check the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df1af9-fa0a-45b0-ae18-e6c5bd831abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(f'{k}, {v.shape}, {v.type()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136feacc-75fb-4814-8827-c05fada1f4fe",
   "metadata": {},
   "source": [
    "The training data is fed to the Encoder stage of transformer. In contrast to the NLP task where Encoder **is not** fed with `input_ids` and `attention_mask` (padding varying lenghts of samples). Instead we are sending :\n",
    "- `past_values`                 -- input time points `context_length` in length\n",
    "- `past_observed_mask`          -- (masking NaN's and 0s from numerics which are set to 1s)\n",
    "- `past_time_features`          -- time features added to the `past_values` during transformations, e.g. `month of the year` and `age` in our case   \n",
    "- `static_categorical_features`  -- the index in a time series that doesn't change, in our case it is `time_series_id`. For this the model will learn embedding of the dimension `embedding_dimension`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc30c9a-889b-488f-a3b7-b669c022a1d5",
   "metadata": {},
   "source": [
    "For the decoder inputs consist of:\n",
    "\n",
    "- `future_observed_mask`:Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected in [0, 1]:\n",
    "   - 1 for values that are observed,\n",
    "   - 0 for values that are missing (i.e. NaNs that were replaced by zeros).\n",
    "This mask is used to filter out missing values for the final loss calculation.\n",
    "- `future_time_features`: these features will be add during transformations, e.g. `month of the year` and `age` in our case. They are `prediction_length` long.\n",
    "- `future_values`: future values of the time series, that serve as labels for the model. The `future_values` is what the Transformer needs *during training to learn to output*, given the past_values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1db37-7071-4a25-9a39-3486b85c1ca6",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "Let's test a single forward pass with the batch we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2b36a-cccb-4084-ab03-06004b330c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform forward pass\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"]\n",
    "    if config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=batch[\"static_real_features\"]\n",
    "    if config.num_static_real_features > 0\n",
    "    else None,\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    "    future_observed_mask=batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e2d10-2b44-4607-9e6a-55a09a18ab8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "print(\"Loss:\", outputs.loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d6bd1-de14-4959-b561-08c08e2acd7f",
   "metadata": {},
   "source": [
    "Note that the model is returning a loss. This is possible as the decoder automatically shifts the `future_values` one position to the right in order to have the labels. This allows computing a loss between the predicted values and the labels.\n",
    "\n",
    "Also note that the decoder uses a causal mask (to not look into the future as the values it needs to predict are in the `future_values` tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b0202-4865-44d7-8b58-1f4ae224f019",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Model\n",
    "Now that our `batch` creation and Forward pass appears to be working, let's try and train our model.\n",
    "\n",
    "To make use of all 8 CPUs available in your notebook instance, we will use ü§ó [Accelerate](https://huggingface.co/docs/accelerate/index) library. It is a convinient way to make the training fast. The library guess an accelerator e.g. a GPU, is available, otherwise falls back on CPU training.  \n",
    "\n",
    "Here we are using **AdamW** (Adaptive Moment Estimation) as our optimizer. As opposed to stochastic gradient decent (SGD), AdamW converges faster whereas SGD gives more optimal solution. The intuition behind the Adam that it modulates the momentums (first and second order) to adaptive change the it.\n",
    "(Momentum is an additional term added to the gradient to help move the algorithm faste and avoid stalls in local minima, a classical issue with SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89d473-09c3-4579-8f38-7d49f14e518c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# setup accelerator discovery\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# load model to GPU if its there\n",
    "model.to(device)\n",
    "# initialize optimizer with hyperparameters\n",
    "optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "# wrap our model, optimizer and data_loader with Accelerate's wrapper. \n",
    "# The distribution will be taken care of by Accelerate library \n",
    "model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    ")\n",
    "\n",
    "# set mode to training. PyTorch will enable use of e.g. dropout or Batch_normalization()\n",
    "# to avoid overfitting. It also expcets backward pass in this mode.\n",
    "# These functionalities are not used during inference.\n",
    "model.train()\n",
    "\n",
    "# iterate for 10 epochs -- an epoch the model has been \n",
    "# presented all the tokens in dataset once. \n",
    "\n",
    "for epoch in range(10):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(device)\n",
    "            if config.num_static_real_features > 0\n",
    "            else None,\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            future_values=batch[\"future_values\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f'{epoch}: Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf5a3b-8d49-4495-9273-66e2fec9492a",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "At inference time, it's recommended to use the `generate()` method for autoregressive generation, similar to NLP models.\n",
    "\n",
    "Forecasting involves getting data from the test instance sampler, which will sample the very last `context_length` sized window of values from each time series in the dataset, and pass it to the model. Note that we pass `future_time_features`, which are known ahead of time, to the decoder.\n",
    "\n",
    "The model will autoregressively sample a certain number of values from the predicted distribution and pass them back to the decoder to return the prediction outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11517e-83ee-4f56-af0c-4d5cc3fd8902",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the mode to eval for inference\n",
    "model.eval()\n",
    "\n",
    "forecasts = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "        if config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(device)\n",
    "        if config.num_static_real_features > 0\n",
    "        else None,\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "    )\n",
    "    forecasts.append(outputs.sequences.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57754229-1fa9-4199-9d6c-b9a3674d4eb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`).\n",
    "\n",
    "In this case, we get `100` possible values for the next `24` months (for each example in the batch which is of size `64`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2820dd-41da-4199-a6b2-12739c465cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forecasts[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeba2e2-3c96-4742-9ae5-10fcf1e4b3f6",
   "metadata": {},
   "source": [
    "We'll stack them vertically, to get forecasts for all time-series in the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7534898-1b05-4359-920b-43f9d836d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = np.vstack(forecasts)\n",
    "print(forecasts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8058b-c7cd-4860-a473-c8c4ea61d85d",
   "metadata": {},
   "source": [
    "### Quality metric \n",
    "We can evaluate the resulting forecast with respect to the ground truth out of sample values present in the test set. For that, we'll use the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library, which includes the [MASE](https://huggingface.co/spaces/evaluate-metric/mase) and [sMAPE](https://huggingface.co/spaces/evaluate-metric/smape) metrics.\n",
    "\n",
    "#### MASE\n",
    "\n",
    "Mean Absolute Scaled Error is the mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast.\n",
    "Interpretable, as values greater than one indicate that in-sample one-step forecasts from the na√Øve method perform better than the forecast values under consideration.\n",
    "\n",
    "#### sMAPE\n",
    "\n",
    "Symmetric Mean Absolute Percentage Error is the symmetric mean percentage error difference between the predicted and actual values.\n",
    "\n",
    "This metric is called a measure of ‚Äúpercentage error‚Äù even though there is no multiplier of 100. The range is between (0, 2) with it being 2 when the target and prediction are both zero.\n",
    "\n",
    "**Therefore in both metrics cases, lower is better** \n",
    "\n",
    "We calculate both metrics for each time series in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90903dbf-4d27-4547-8507-889a696f3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "smape_metric = load(\"evaluate-metric/smape\")\n",
    "\n",
    "forecast_median = np.median(forecasts, 1)\n",
    "\n",
    "mase_metrics = []\n",
    "smape_metrics = []\n",
    "for item_id, ts in enumerate(test_dataset):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "    mase = mase_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "        training=np.array(training_data),\n",
    "        periodicity=get_seasonality(freq),\n",
    "    )\n",
    "    mase_metrics.append(mase[\"mase\"])\n",
    "\n",
    "    smape = smape_metric.compute(\n",
    "        predictions=forecast_median[item_id],\n",
    "        references=np.array(ground_truth),\n",
    "    )\n",
    "    smape_metrics.append(smape[\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7bdc8-f25b-4baf-90e5-476db38892d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MASE: {np.mean(mase_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ec816-4dd2-45e7-afd2-df5781056b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sMAPE: {np.mean(smape_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6878717-1c9b-4a34-a202-3cd2b5f90885",
   "metadata": {},
   "source": [
    "We can also plot the individual metrics of each time series in the dataset and observe that a handful of time series contribute a lot to the final test metric:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82090ef6-6934-47d4-a149-09a3d2cbd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(mase_metrics, smape_metrics, alpha=0.3)\n",
    "plt.xlabel(\"MASE\")\n",
    "plt.ylabel(\"sMAPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf3643-dccb-4c5e-9195-d5d3aa2844d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "def plot(ts_index):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = pd.period_range(\n",
    "        start=test_dataset[ts_index][FieldName.START],\n",
    "        periods=len(test_dataset[ts_index][FieldName.TARGET]),\n",
    "        freq=freq,\n",
    "    ).to_timestamp()\n",
    "\n",
    "    # Major ticks every half year, minor ticks every month,\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 7)))\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "\n",
    "    ax.plot(\n",
    "        index[-2 * prediction_length :],\n",
    "        test_dataset[ts_index][\"target\"][-2 * prediction_length :],\n",
    "        label=\"actual\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        index[-prediction_length:],\n",
    "        np.median(forecasts[ts_index], axis=0),\n",
    "        label=\"median\",\n",
    "    )\n",
    "\n",
    "    plt.fill_between(\n",
    "        index[-prediction_length:],\n",
    "        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),\n",
    "        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),\n",
    "        alpha=0.3,\n",
    "        interpolate=True,\n",
    "        label=\"+/- 1-std\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767e0b3-e91f-43ed-8f1c-bde3e51c1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(plot(len(train_dataset))-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
